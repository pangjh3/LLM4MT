""" PyTorch LLaMA model."""
import math
from typing import List, Optional, Tuple, Union
import transformers
import torch
import torch.utils.checkpoint
from torch import nn
from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss

from transformers.activations import ACT2FN
from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast
from transformers.modeling_utils import PreTrainedModel
from transformers.utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings
from transformers.models.llama.configuration_llama import LlamaConfig
import numpy as np

logger = logging.get_logger(__name__)

_CONFIG_FOR_DOC = "LlamaConfig"

AC_TOKEN_ID=32001
# PAD_TOKEN_ID=32000

def llamamodel_forward_noflashattn_onlykvcache(
    self,
    input_ids: torch.LongTensor = None,
    attention_mask: Optional[torch.Tensor] = None,
    past_key_values: Optional[List[torch.FloatTensor]] = None,
    inputs_embeds: Optional[torch.FloatTensor] = None,
    use_cache: Optional[bool] = None,
    output_attentions: Optional[bool] = None,
    output_hidden_states: Optional[bool] = None,
    return_dict: Optional[bool] = None,
) -> Union[Tuple, BaseModelOutputWithPast]:
    r"""
    Args:
        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you
            provide it.

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            [What are input IDs?](../glossary#input-ids)
        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.

            [What are attention masks?](../glossary#attention-mask)
        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of

            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the
            cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.

            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those
            that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of
            all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
        use_cache (`bool`, *optional*):
            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding
            (see `past_key_values`).
        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.
            This is useful if you want more control over how to convert `input_ids` indices into associated vectors
            than the model's internal embedding lookup matrix.
        output_attentions (`bool`, *optional*):
            Whether or not to return the attentions tensors of all attention layers. See `attentions` under
            returned tensors for more detail.
        output_hidden_states (`bool`, *optional*):
            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors
            for more detail.
        return_dict (`bool`, *optional*):
            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
    """
    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
    output_hidden_states = (
        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
    )
    use_cache = use_cache if use_cache is not None else self.config.use_cache

    return_dict = return_dict if return_dict is not None else self.config.use_return_dict

    # retrieve input_ids and inputs_embeds
    if input_ids is not None and inputs_embeds is not None:
        raise ValueError("You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time")
    elif input_ids is not None:
        batch_size, seq_length = input_ids.shape
    elif inputs_embeds is not None:
        batch_size, seq_length, _ = inputs_embeds.shape
    else:
        raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
    seq_length_with_past = seq_length
    past_key_values_length = 0
    if past_key_values is not None:
        past_key_values_length = past_key_values[0][0].shape[2]
        seq_length_with_past = seq_length_with_past + past_key_values_length
    if inputs_embeds is None:
        inputs_embeds = self.embed_tokens(input_ids)
    # embed positions
    if attention_mask is None:
        attention_mask = torch.ones(
            (batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device
        )
    attention_mask = self._prepare_decoder_attention_mask(
        attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length
    )

    global AC_TOKEN_ID
    # attn mask by pjh
    datatype=inputs_embeds.dtype
    datadevice=inputs_embeds.device
    pastpkvi = []
    assert batch_size == 1
    hasanchor=False
    inputi = input_ids[0]

    # if past_key_values is not None:
    pastpkvi = [i for i in range(past_key_values_length)]
    first=False
    if len(pastpkvi) == 0:
        # print("first")
        first=True

    indextruefalse = inputi==AC_TOKEN_ID[0]
    for idd in AC_TOKEN_ID[1:]:
        indextruefalse += (inputi==idd)
    if torch.sum(indextruefalse) != 0:
        if first:
            hasanchor=True
        acindex = torch.nonzero(indextruefalse).squeeze().tolist()
        start=0
        if not isinstance(acindex, list):
            acindex = [acindex]
        for ix in acindex:
            mask[ix+1:,start:ix] = torch.finfo(datatype).min
            start=ix+1
    
    # padindex = torch.nonzero(inputi==PAD_TOKEN_ID).squeeze().tolist()
    # mask[padindex,:] = torch.finfo(datatype).min
    # mask[:,padindex] = torch.finfo(datatype).min\

        # for past key value index
        pastpkvi.extend([i for i in acindex])
        # pastpkvi.extend(past_key_values_length+acindex)
        maxacindex = acindex[-1]
        pastpkvi.extend([i for i in range(maxacindex+1, seq_length)])
        # if first:
        #     pastpkvi.extend([i for i in range(maxacindex+1, seq_length)])
        #     print(pastpkvi)
        # else:
        #     print('-',pastpkvi)

    


    if past_key_values_length > 0:
        mask = torch.cat([torch.zeros(seq_length, past_key_values_length, dtype=datatype, device=datadevice), mask], dim=-1)

    # attention_mask.append(mask.unsqueeze(0))

    # attention_mask = torch.cat(attention_mask, dim=0).to(inputs_embeds).to(datadevice)
    attention_mask = mask[None, None, :, :].expand(1, 1, seq_length, seq_length+past_key_values_length)
    # attention_mask = attention_mask.unsqueeze(1)
    # print(attention_mask)
    # print(attention_mask.size())

    hidden_states = inputs_embeds

    if self.gradient_checkpointing and self.training:
        if use_cache:
            logger.warning_once(
                "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
            )
            use_cache = False

    # decoder layers
    all_hidden_states = () if output_hidden_states else None
    all_self_attns = () if output_attentions else None
    next_decoder_cache = () if use_cache else None

    for idx, decoder_layer in enumerate(self.layers):
        if output_hidden_states:
            all_hidden_states += (hidden_states,)

        past_key_value = past_key_values[idx] if past_key_values is not None else None

        if self.gradient_checkpointing and self.training:

            def create_custom_forward(module):
                def custom_forward(*inputs):
                    # None for past_key_value
                    return module(*inputs, output_attentions, None)

                return custom_forward

            layer_outputs = torch.utils.checkpoint.checkpoint(
                create_custom_forward(decoder_layer),
                hidden_states,
                attention_mask,
                None,
            )
        else:
            layer_outputs = decoder_layer(
                hidden_states,
                attention_mask=attention_mask,
                past_key_value=past_key_value,
                output_attentions=output_attentions,
                use_cache=use_cache,
            )

        hidden_states = layer_outputs[0]
        if use_cache:
            pkvi = layer_outputs[2 if output_attentions else 1]
            if hasanchor:
                next_decoder_cache += ((pkvi[0][:,:,pastpkvi,:], pkvi[1][:,:,pastpkvi,:]),)
                # print(next_decoder_cache)
                # print(idx, next_decoder_cache[0].size(), next_decoder_cache[1].size())
            else:
                # print(pkvi)
                next_decoder_cache += (pkvi,)

        if output_attentions:
            all_self_attns += (layer_outputs[1],)

    hidden_states = self.norm(hidden_states)

    # add hidden states from the last decoder layer
    if output_hidden_states:
        all_hidden_states += (hidden_states,)

    next_cache = next_decoder_cache if use_cache else None
    if not return_dict:
        return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
    return BaseModelOutputWithPast(
        last_hidden_state=hidden_states,
        past_key_values=next_cache,
        hidden_states=all_hidden_states,
        attentions=all_self_attns,
    )


def llamamodel_forward_noflashattn(
    self,
    input_ids: torch.LongTensor = None,
    attention_mask: Optional[torch.Tensor] = None,
    past_key_values: Optional[List[torch.FloatTensor]] = None,
    inputs_embeds: Optional[torch.FloatTensor] = None,
    use_cache: Optional[bool] = None,
    output_attentions: Optional[bool] = None,
    output_hidden_states: Optional[bool] = None,
    return_dict: Optional[bool] = None,
) -> Union[Tuple, BaseModelOutputWithPast]:
    r"""
    Args:
        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you
            provide it.

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            [What are input IDs?](../glossary#input-ids)
        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.

            [What are attention masks?](../glossary#attention-mask)
        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of

            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the
            cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.

            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those
            that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of
            all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
        use_cache (`bool`, *optional*):
            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding
            (see `past_key_values`).
        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.
            This is useful if you want more control over how to convert `input_ids` indices into associated vectors
            than the model's internal embedding lookup matrix.
        output_attentions (`bool`, *optional*):
            Whether or not to return the attentions tensors of all attention layers. See `attentions` under
            returned tensors for more detail.
        output_hidden_states (`bool`, *optional*):
            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors
            for more detail.
        return_dict (`bool`, *optional*):
            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
    """
    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
    output_hidden_states = (
        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
    )
    use_cache = use_cache if use_cache is not None else self.config.use_cache

    return_dict = return_dict if return_dict is not None else self.config.use_return_dict

    # retrieve input_ids and inputs_embeds
    if input_ids is not None and inputs_embeds is not None:
        raise ValueError("You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time")
    elif input_ids is not None:
        batch_size, seq_length = input_ids.shape
    elif inputs_embeds is not None:
        batch_size, seq_length, _ = inputs_embeds.shape
    else:
        raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
    seq_length_with_past = seq_length
    past_key_values_length = 0
    if past_key_values is not None:
        past_key_values_length = past_key_values[0][0].shape[2]
        seq_length_with_past = seq_length_with_past + past_key_values_length
    if inputs_embeds is None:
        inputs_embeds = self.embed_tokens(input_ids)
    # embed positions
    # if attention_mask is None:
    #     attention_mask = torch.ones(
    #         (batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device
    #     )
    # attention_mask = self._prepare_decoder_attention_mask(
    #     attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length
    # )

    # attn mask by pjh
    global AC_TOKEN_ID

    datatype=inputs_embeds.dtype
    datadevice=inputs_embeds.device
    attention_mask = []
    for i in range(batch_size):
        inputi = input_ids[i]
        mask = torch.full((seq_length, seq_length), torch.finfo(datatype).min, device=datadevice)
        mask_cond = torch.arange(mask.size(-1), device=datadevice)
        mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)

        if not isinstance(AC_TOKEN_ID, list):
            AC_TOKEN_ID = [AC_TOKEN_ID]
        
        indextruefalse = inputi==AC_TOKEN_ID[0]
        for idd in AC_TOKEN_ID[1:]:
            indextruefalse += (inputi==idd)
        if torch.sum(indextruefalse) != 0:
            acindex = torch.nonzero(indextruefalse).squeeze().tolist()
            start=0
            if not isinstance(acindex, list):
                acindex = [acindex]
                
            for ix in acindex:
                mask[ix+1:,start:ix] = torch.finfo(datatype).min
                start=ix+1
        
        # padindex = torch.nonzero(inputi==PAD_TOKEN_ID).squeeze().tolist()
        # mask[padindex,:] = torch.finfo(datatype).min
        # mask[:,padindex] = torch.finfo(datatype).min

        if past_key_values_length > 0:
            mask = torch.cat([torch.zeros(seq_length, past_key_values_length, dtype=datatype, device=datadevice), mask], dim=-1)

        attention_mask.append(mask.unsqueeze(0))

    attention_mask = torch.cat(attention_mask, dim=0).to(inputs_embeds).to(datadevice)
    # attention_mask = mask[None, None, :, :].expand(2, 1, seq_length, seq_length+past_key_values_length)
    attention_mask = attention_mask.unsqueeze(1)


    hidden_states = inputs_embeds

    if self.gradient_checkpointing and self.training:
        if use_cache:
            logger.warning_once(
                "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
            )
            use_cache = False

    # decoder layers
    all_hidden_states = () if output_hidden_states else None
    all_self_attns = () if output_attentions else None
    next_decoder_cache = () if use_cache else None

    for idx, decoder_layer in enumerate(self.layers):
        if output_hidden_states:
            all_hidden_states += (hidden_states,)

        past_key_value = past_key_values[idx] if past_key_values is not None else None

        if self.gradient_checkpointing and self.training:

            def create_custom_forward(module):
                def custom_forward(*inputs):
                    # None for past_key_value
                    return module(*inputs, output_attentions, None)

                return custom_forward

            layer_outputs = torch.utils.checkpoint.checkpoint(
                create_custom_forward(decoder_layer),
                hidden_states,
                attention_mask,
                None,
            )
        else:
            layer_outputs = decoder_layer(
                hidden_states,
                attention_mask=attention_mask,
                past_key_value=past_key_value,
                output_attentions=output_attentions,
                use_cache=use_cache,
            )

        hidden_states = layer_outputs[0]

        if use_cache:
            next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)

        if output_attentions:
            all_self_attns += (layer_outputs[1],)

    hidden_states = self.norm(hidden_states)

    # add hidden states from the last decoder layer
    if output_hidden_states:
        all_hidden_states += (hidden_states,)

    next_cache = next_decoder_cache if use_cache else None
    if not return_dict:
        return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
    return BaseModelOutputWithPast(
        last_hidden_state=hidden_states,
        past_key_values=next_cache,
        hidden_states=all_hidden_states,
        attentions=all_self_attns,
    )

def replace_llama_forward(ac_tokenid):
    # global AC_TOKEN_ID, PAD_TOKEN_ID
    global AC_TOKEN_ID
    AC_TOKEN_ID = ac_tokenid
    # PAD_TOKEN_ID = pad_tokenid
    transformers.models.llama.modeling_llama.LlamaModel.forward = llamamodel_forward_noflashattn



def llamamodel_forward_noflashattn_fast(
    self,
    input_ids: torch.LongTensor = None,
    attention_mask: Optional[torch.Tensor] = None,
    past_key_values: Optional[List[torch.FloatTensor]] = None,
    inputs_embeds: Optional[torch.FloatTensor] = None,
    use_cache: Optional[bool] = None,
    output_attentions: Optional[bool] = None,
    output_hidden_states: Optional[bool] = None,
    return_dict: Optional[bool] = None,
) -> Union[Tuple, BaseModelOutputWithPast]:
    r"""
    Args:
        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you
            provide it.

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            [What are input IDs?](../glossary#input-ids)
        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.

            [What are attention masks?](../glossary#attention-mask)
        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of

            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the
            cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.

            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those
            that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of
            all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
        use_cache (`bool`, *optional*):
            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding
            (see `past_key_values`).
        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.
            This is useful if you want more control over how to convert `input_ids` indices into associated vectors
            than the model's internal embedding lookup matrix.
        output_attentions (`bool`, *optional*):
            Whether or not to return the attentions tensors of all attention layers. See `attentions` under
            returned tensors for more detail.
        output_hidden_states (`bool`, *optional*):
            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors
            for more detail.
        return_dict (`bool`, *optional*):
            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
    """
    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
    output_hidden_states = (
        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
    )
    use_cache = use_cache if use_cache is not None else self.config.use_cache

    return_dict = return_dict if return_dict is not None else self.config.use_return_dict

    # retrieve input_ids and inputs_embeds
    if input_ids is not None and inputs_embeds is not None:
        raise ValueError("You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time")
    elif input_ids is not None:
        batch_size, seq_length = input_ids.shape
    elif inputs_embeds is not None:
        batch_size, seq_length, _ = inputs_embeds.shape
    else:
        raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
    seq_length_with_past = seq_length
    past_key_values_length = 0
    if past_key_values is not None:
        past_key_values_length = past_key_values[0][0].shape[2]
        seq_length_with_past = seq_length_with_past + past_key_values_length
    if inputs_embeds is None:
        inputs_embeds = self.embed_tokens(input_ids)
    # embed positions

    global AC_TOKEN_ID
    # attn mask by pjh
    datatype=inputs_embeds.dtype
    datadevice=inputs_embeds.device
    pastpkvi = []

    assert batch_size == 1
    hasanchor=False

    # if past_key_values is not None:
    #     if attention_mask is None:
    #         attention_mask = torch.ones(
    #             (batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device
    #         )
    #     attention_mask = self._prepare_decoder_attention_mask(
    #         attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length
    #     )
    # else:
    attention_mask = []
    # for i in range(batch_size):
    inputi = input_ids[0]
    # print(inputi)
    mask = torch.full((seq_length, seq_length), torch.finfo(datatype).min, device=datadevice)
    mask_cond = torch.arange(mask.size(-1), device=datadevice)
    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)

    # if past_key_values is not None:
    pastpkvi = [i for i in range(past_key_values_length)]
    first=False
    if len(pastpkvi) == 0:
        # print("first")
        first=True

    indextruefalse = inputi==AC_TOKEN_ID[0]
    for idd in AC_TOKEN_ID[1:]:
        indextruefalse += (inputi==idd)
    if torch.sum(indextruefalse) != 0:
        if first:
            hasanchor=True
        acindex = torch.nonzero(indextruefalse).squeeze().tolist()
        start=0
        if not isinstance(acindex, list):
            acindex = [acindex]
        for ix in acindex:
            mask[ix+1:,start:ix] = torch.finfo(datatype).min
            start=ix+1
    
    # padindex = torch.nonzero(inputi==PAD_TOKEN_ID).squeeze().tolist()
    # mask[padindex,:] = torch.finfo(datatype).min
    # mask[:,padindex] = torch.finfo(datatype).min\

        # for past key value index
        pastpkvi.extend([i+past_key_values_length for i in acindex])
        # pastpkvi.extend(past_key_values_length+acindex)
        maxacindex = acindex[-1]
        pastpkvi.extend([i for i in range(maxacindex+1, seq_length)])
        # if first:
        #     pastpkvi.extend([i for i in range(maxacindex+1, seq_length)])
        #     print(pastpkvi)
        # else:
        #     print('-',pastpkvi)

    


    if past_key_values_length > 0:
        mask = torch.cat([torch.zeros(seq_length, past_key_values_length, dtype=datatype, device=datadevice), mask], dim=-1)

    # attention_mask.append(mask.unsqueeze(0))

    # attention_mask = torch.cat(attention_mask, dim=0).to(inputs_embeds).to(datadevice)
    attention_mask = mask[None, None, :, :].expand(1, 1, seq_length, seq_length+past_key_values_length)
    # attention_mask = attention_mask.unsqueeze(1)
    # print(attention_mask)
    # print(attention_mask.size())

    hidden_states = inputs_embeds

    if self.gradient_checkpointing and self.training:
        if use_cache:
            logger.warning_once(
                "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
            )
            use_cache = False

    # decoder layers
    all_hidden_states = () if output_hidden_states else None
    all_self_attns = () if output_attentions else None
    next_decoder_cache = () if use_cache else None

    for idx, decoder_layer in enumerate(self.layers):
        if output_hidden_states:
            all_hidden_states += (hidden_states,)

        past_key_value = past_key_values[idx] if past_key_values is not None else None

        if self.gradient_checkpointing and self.training:

            def create_custom_forward(module):
                def custom_forward(*inputs):
                    # None for past_key_value
                    return module(*inputs, output_attentions, None)

                return custom_forward

            layer_outputs = torch.utils.checkpoint.checkpoint(
                create_custom_forward(decoder_layer),
                hidden_states,
                attention_mask,
                None,
            )
        else:
            layer_outputs = decoder_layer(
                hidden_states,
                attention_mask=attention_mask,
                past_key_value=past_key_value,
                output_attentions=output_attentions,
                use_cache=use_cache,
            )

        hidden_states = layer_outputs[0]
        if use_cache:
            pkvi = layer_outputs[2 if output_attentions else 1]
            if hasanchor:
                next_decoder_cache += ((pkvi[0][:,:,pastpkvi,:], pkvi[1][:,:,pastpkvi,:]),)
                # print(next_decoder_cache)
                # print(idx, next_decoder_cache[0].size(), next_decoder_cache[1].size())
            else:
                # print(pkvi)
                next_decoder_cache += (pkvi,)

        if output_attentions:
            all_self_attns += (layer_outputs[1],)

    hidden_states = self.norm(hidden_states)

    # add hidden states from the last decoder layer
    if output_hidden_states:
        all_hidden_states += (hidden_states,)

    next_cache = next_decoder_cache if use_cache else None
    if not return_dict:
        return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
    return BaseModelOutputWithPast(
        last_hidden_state=hidden_states,
        past_key_values=next_cache,
        hidden_states=all_hidden_states,
        attentions=all_self_attns,
    )


def llamamodel_forward_noflashattn_fast2(
    self,
    input_ids: torch.LongTensor = None,
    attention_mask: Optional[torch.Tensor] = None,
    past_key_values: Optional[List[torch.FloatTensor]] = None,
    inputs_embeds: Optional[torch.FloatTensor] = None,
    use_cache: Optional[bool] = None,
    output_attentions: Optional[bool] = None,
    output_hidden_states: Optional[bool] = None,
    return_dict: Optional[bool] = None,
) -> Union[Tuple, BaseModelOutputWithPast]:
    r"""
    Args:
        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you
            provide it.

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            [What are input IDs?](../glossary#input-ids)
        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.

            [What are attention masks?](../glossary#attention-mask)
        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of

            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the
            cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.

            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those
            that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of
            all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
        use_cache (`bool`, *optional*):
            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding
            (see `past_key_values`).
        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.
            This is useful if you want more control over how to convert `input_ids` indices into associated vectors
            than the model's internal embedding lookup matrix.
        output_attentions (`bool`, *optional*):
            Whether or not to return the attentions tensors of all attention layers. See `attentions` under
            returned tensors for more detail.
        output_hidden_states (`bool`, *optional*):
            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors
            for more detail.
        return_dict (`bool`, *optional*):
            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
    """
    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
    output_hidden_states = (
        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
    )
    use_cache = use_cache if use_cache is not None else self.config.use_cache

    return_dict = return_dict if return_dict is not None else self.config.use_return_dict

    # retrieve input_ids and inputs_embeds
    if input_ids is not None and inputs_embeds is not None:
        raise ValueError("You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time")
    elif input_ids is not None:
        batch_size, seq_length = input_ids.shape
    elif inputs_embeds is not None:
        batch_size, seq_length, _ = inputs_embeds.shape
    else:
        raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
    seq_length_with_past = seq_length
    past_key_values_length = 0
    if past_key_values is not None:
        past_key_values_length = past_key_values[0][0].shape[2]
        seq_length_with_past = seq_length_with_past + past_key_values_length
    if inputs_embeds is None:
        inputs_embeds = self.embed_tokens(input_ids)
    # embed positions
    # if attention_mask is None:
    #     attention_mask = torch.ones(
    #         (batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device
    #     )
    # attention_mask = self._prepare_decoder_attention_mask(
    #     attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length
    # )

    global AC_TOKEN_ID
    # attn mask by pjh
    datatype=inputs_embeds.dtype
    datadevice=inputs_embeds.device
    attention_mask = []
    pastpkvi = []

    assert batch_size == 1
    hasanchor=False
    for i in range(batch_size):
        inputi = input_ids[i]
        mask = torch.full((seq_length, seq_length), torch.finfo(datatype).min, device=datadevice)
        mask_cond = torch.arange(mask.size(-1), device=datadevice)
        mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)

        # if past_key_values is not None:
        pastpkvi = [i for i in range(past_key_values_length)]

        indextruefalse = inputi==AC_TOKEN_ID[0]
        for idd in AC_TOKEN_ID[1:]:
            indextruefalse += (inputi==idd)
        if torch.sum(indextruefalse) != 0:
            hasanchor=True
            acindex = torch.nonzero(indextruefalse).squeeze().tolist()
            start=0
            if not isinstance(acindex, list):
                acindex = [acindex]

            # for ix in acindex:
            #     mask[ix+1:,start:ix] = torch.finfo(datatype).min
            #     start=ix+1
        
        # padindex = torch.nonzero(inputi==PAD_TOKEN_ID).squeeze().tolist()
        # mask[padindex,:] = torch.finfo(datatype).min
        # mask[:,padindex] = torch.finfo(datatype).min\

            # for past key value index
            pastpkvi.extend([i+past_key_values_length for i in acindex])
            # pastpkvi.extend(past_key_values_length+acindex)


        if past_key_values_length > 0:
            mask = torch.cat([torch.zeros(seq_length, past_key_values_length, dtype=datatype, device=datadevice), mask], dim=-1)

        attention_mask.append(mask.unsqueeze(0))

    attention_mask = torch.cat(attention_mask, dim=0).to(inputs_embeds).to(datadevice)
    # attention_mask = mask[None, None, :, :].expand(2, 1, seq_length, seq_length+past_key_values_length)
    attention_mask = attention_mask.unsqueeze(1)


    hidden_states = inputs_embeds

    if self.gradient_checkpointing and self.training:
        if use_cache:
            logger.warning_once(
                "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
            )
            use_cache = False

    # decoder layers
    all_hidden_states = () if output_hidden_states else None
    all_self_attns = () if output_attentions else None
    next_decoder_cache = () if use_cache else None

    for idx, decoder_layer in enumerate(self.layers):
        if output_hidden_states:
            all_hidden_states += (hidden_states,)

        past_key_value = past_key_values[idx] if past_key_values is not None else None

        if self.gradient_checkpointing and self.training:

            def create_custom_forward(module):
                def custom_forward(*inputs):
                    # None for past_key_value
                    return module(*inputs, output_attentions, None)

                return custom_forward

            layer_outputs = torch.utils.checkpoint.checkpoint(
                create_custom_forward(decoder_layer),
                hidden_states,
                attention_mask,
                None,
            )
        else:
            layer_outputs = decoder_layer(
                hidden_states,
                attention_mask=attention_mask,
                past_key_value=past_key_value,
                output_attentions=output_attentions,
                use_cache=use_cache,
            )

        hidden_states = layer_outputs[0]
        if use_cache:
            pkvi = layer_outputs[2 if output_attentions else 1]
            if hasanchor:
                next_decoder_cache += ((pkvi[0][:,:,pastpkvi,:], pkvi[1][:,:,pastpkvi,:]),)
                # print(next_decoder_cache)
                # print(idx, next_decoder_cache[0].size(), next_decoder_cache[1].size())
            else:
                # print(pkvi)
                next_decoder_cache += (pkvi,)

        if output_attentions:
            all_self_attns += (layer_outputs[1],)

    hidden_states = self.norm(hidden_states)

    # add hidden states from the last decoder layer
    if output_hidden_states:
        all_hidden_states += (hidden_states,)

    next_cache = next_decoder_cache if use_cache else None
    if not return_dict:
        return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
    return BaseModelOutputWithPast(
        last_hidden_state=hidden_states,
        past_key_values=next_cache,
        hidden_states=all_hidden_states,
        attentions=all_self_attns,
    )


def llamamodel_forward_noflashattn_fast_realtime(
    self,
    input_ids: torch.LongTensor = None,
    attention_mask: Optional[torch.Tensor] = None,
    past_key_values: Optional[List[torch.FloatTensor]] = None,
    inputs_embeds: Optional[torch.FloatTensor] = None,
    use_cache: Optional[bool] = None,
    output_attentions: Optional[bool] = None,
    output_hidden_states: Optional[bool] = None,
    return_dict: Optional[bool] = None,
) -> Union[Tuple, BaseModelOutputWithPast]:
    r"""
    Args:
        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you
            provide it.

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            [What are input IDs?](../glossary#input-ids)
        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.

            [What are attention masks?](../glossary#attention-mask)
        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of

            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the
            cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.

            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those
            that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of
            all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
        use_cache (`bool`, *optional*):
            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding
            (see `past_key_values`).
        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.
            This is useful if you want more control over how to convert `input_ids` indices into associated vectors
            than the model's internal embedding lookup matrix.
        output_attentions (`bool`, *optional*):
            Whether or not to return the attentions tensors of all attention layers. See `attentions` under
            returned tensors for more detail.
        output_hidden_states (`bool`, *optional*):
            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors
            for more detail.
        return_dict (`bool`, *optional*):
            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
    """
    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
    output_hidden_states = (
        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
    )
    use_cache = use_cache if use_cache is not None else self.config.use_cache

    return_dict = return_dict if return_dict is not None else self.config.use_return_dict

    # retrieve input_ids and inputs_embeds
    if input_ids is not None and inputs_embeds is not None:
        raise ValueError("You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time")
    elif input_ids is not None:
        batch_size, seq_length = input_ids.shape
    elif inputs_embeds is not None:
        batch_size, seq_length, _ = inputs_embeds.shape
    else:
        raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
    seq_length_with_past = seq_length
    past_key_values_length = 0
    if past_key_values is not None:
        past_key_values_length = past_key_values[0][0].shape[2]
        seq_length_with_past = seq_length_with_past + past_key_values_length
    if inputs_embeds is None:
        inputs_embeds = self.embed_tokens(input_ids)
    # embed positions
    # if attention_mask is None:
    #     attention_mask = torch.ones(
    #         (batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device
    #     )
    # attention_mask = self._prepare_decoder_attention_mask(
    #     attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length
    # )

    global AC_TOKEN_ID
    # attn mask by pjh
    datatype=inputs_embeds.dtype
    datadevice=inputs_embeds.device
    attention_mask = []
    pastpkvi = []

    # if not getattr(self, "acindex", False):
    #     self.acindex = []
    
    assert batch_size == 1
    hasanchor=False
    for i in range(batch_size):
        inputi = input_ids[i]
        mask = torch.full((seq_length, seq_length), torch.finfo(datatype).min, device=datadevice)
        mask_cond = torch.arange(mask.size(-1), device=datadevice)
        mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)

        # if past_key_values is not None:
        pastpkvi = [i for i in range(past_key_values_length)]
        pastanchorpkv = self.acindex

        indextruefalse = inputi==AC_TOKEN_ID[0]
        for idd in AC_TOKEN_ID[1:]:
            indextruefalse += (inputi==idd)
            
        if torch.sum(indextruefalse) != 0:
            hasanchor=True
            acindex = torch.nonzero(indextruefalse).squeeze().tolist()
            start=0
            if not isinstance(acindex, list):
                acindex = [acindex]
    
            for ix in acindex:
                mask[ix+1:,start:ix] = torch.finfo(datatype).min
                start=ix+1
        
        # padindex = torch.nonzero(inputi==PAD_TOKEN_ID).squeeze().tolist()
        # mask[padindex,:] = torch.finfo(datatype).min
        # mask[:,padindex] = torch.finfo(datatype).min

            # for past key value index
            pastpkvi = pastanchorpkv + [i+past_key_values_length for i in acindex]
            self.acindex.extend([i+len(self.acindex) for i in range(len(acindex))])

        if past_key_values_length > 0:
            mask = torch.cat([torch.zeros(seq_length, past_key_values_length, dtype=datatype, device=datadevice), mask], dim=-1)

        attention_mask.append(mask.unsqueeze(0))

    attention_mask = torch.cat(attention_mask, dim=0).to(inputs_embeds).to(datadevice)
    # attention_mask = mask[None, None, :, :].expand(2, 1, seq_length, seq_length+past_key_values_length)
    attention_mask = attention_mask.unsqueeze(1)


    hidden_states = inputs_embeds

    if self.gradient_checkpointing and self.training:
        if use_cache:
            logger.warning_once(
                "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
            )
            use_cache = False

    # decoder layers
    all_hidden_states = () if output_hidden_states else None
    all_self_attns = () if output_attentions else None
    next_decoder_cache = () if use_cache else None

    for idx, decoder_layer in enumerate(self.layers):
        if output_hidden_states:
            all_hidden_states += (hidden_states,)

        past_key_value = past_key_values[idx] if past_key_values is not None else None

        if self.gradient_checkpointing and self.training:

            def create_custom_forward(module):
                def custom_forward(*inputs):
                    # None for past_key_value
                    return module(*inputs, output_attentions, None)

                return custom_forward

            layer_outputs = torch.utils.checkpoint.checkpoint(
                create_custom_forward(decoder_layer),
                hidden_states,
                attention_mask,
                None,
            )
        else:
            layer_outputs = decoder_layer(
                hidden_states,
                attention_mask=attention_mask,
                past_key_value=past_key_value,
                output_attentions=output_attentions,
                use_cache=use_cache,
            )

        hidden_states = layer_outputs[0]
        if use_cache:
            pkvi = layer_outputs[2 if output_attentions else 1]
            if hasanchor:
                next_decoder_cache += ((pkvi[0][:,:,pastpkvi,:], pkvi[1][:,:,pastpkvi,:]),)
                # print(next_decoder_cache)
                # print(idx, next_decoder_cache[0].size(), next_decoder_cache[1].size())
            else:
                # print(pkvi)
                next_decoder_cache += (pkvi,)

        if output_attentions:
            all_self_attns += (layer_outputs[1],)

    hidden_states = self.norm(hidden_states)

    # add hidden states from the last decoder layer
    if output_hidden_states:
        all_hidden_states += (hidden_states,)

    next_cache = next_decoder_cache if use_cache else None
    if not return_dict:
        return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
    return BaseModelOutputWithPast(
        last_hidden_state=hidden_states,
        past_key_values=next_cache,
        hidden_states=all_hidden_states,
        attentions=all_self_attns,
    )


def replace_llama_forward_forfastinfer(ac_tokenid):
    # global AC_TOKEN_ID, PAD_TOKEN_ID
    global AC_TOKEN_ID
    AC_TOKEN_ID = ac_tokenid
    # PAD_TOKEN_ID = pad_tokenid
    # transformers.models.llama.modeling_llama.LlamaModel.forward = llamamodel_forward_noflashattn_fast
    transformers.models.llama.modeling_llama.LlamaModel.forward = llamamodel_forward_noflashattn_fast_realtime
    


def replace_llama_forward_forharness(ac_tokenid):
    # global AC_TOKEN_ID, PAD_TOKEN_ID
    global AC_TOKEN_ID
    AC_TOKEN_ID = ac_tokenid
    # PAD_TOKEN_ID = pad_tokenid
    transformers.models.llama.modeling_llama.LlamaModel.forward = llamamodel_forward_noflashattn_fast
    # transformers.models.llama.modeling_llama.LlamaModel.forward = llamamodel_forward_noflashattn_fast_realtime